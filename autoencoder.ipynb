{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "#import tensorflow_io as tfio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: /physical_device:GPU:0   Type: GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 230)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f824835bd90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.open('UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/Train001/002.tif')\n",
    "im = im.resize((230,150))\n",
    "print(np.array(im).shape)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6800, 100, 100, 1)\n",
      "425\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT=100\n",
    "IMG_WIDTH=100\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE=16\n",
    "\n",
    "train_set = 'UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/*/*.tif'\n",
    "\n",
    "train_files = sorted(glob.glob(train_set))\n",
    "\n",
    "a = np.zeros((len(train_files),IMG_HEIGHT, IMG_WIDTH,1)).astype(np.float32)\n",
    "\n",
    "for idx, filename in enumerate(train_files):\n",
    "    im = Image.open(filename)\n",
    "    im = im.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    a[idx,:,:,0] = np.array(im)/255.0\n",
    "    \n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "train_dataset  = tf.data.Dataset.from_tensor_slices((a, a)).shuffle(1000, \n",
    "                                                                    seed=42, \n",
    "                                                                    reshuffle_each_iteration=False).batch(BATCH_SIZE)\n",
    "print(len(list(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "test_set = 'UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test/Test024/*'\n",
    "\n",
    "test_files = sorted(glob.glob(test_set))\n",
    "\n",
    "a = np.zeros((len(test_files),IMG_WIDTH, IMG_HEIGHT,1)).astype(np.float32)\n",
    "\n",
    "for idx,filename in enumerate(test_files):\n",
    "    im = Image.open(filename)\n",
    "    im = im.resize((IMG_WIDTH,IMG_HEIGHT))\n",
    "    a[idx,:,:,0] = np.array(im, dtype=np.float32)/255.0\n",
    "\n",
    "    \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((a, a)).batch(1)\n",
    "print(len(list(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100, 100, 1) (16, 100, 100, 1)\n",
      "(16, 100, 100, 1) (16, 100, 100, 1)\n",
      "(16, 100, 100, 1) (16, 100, 100, 1)\n",
      "(16, 100, 100, 1) (16, 100, 100, 1)\n",
      "(16, 100, 100, 1) (16, 100, 100, 1)\n",
      "(16, 100, 100, 1) (16, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for x,y in train_dataset.take(BATCH_SIZE):\n",
    "    print(x.shape, y.shape)\n",
    "    count = count + 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100, 100, 1)\n",
      "tf.Tensor(\n",
      "[[0.38039216 0.40392157 0.49019608 ... 0.35686275 0.23921569 0.27058825]\n",
      " [0.34509805 0.41568628 0.48235294 ... 0.34901962 0.3019608  0.2784314 ]\n",
      " [0.4        0.34117648 0.34509805 ... 0.2901961  0.34117648 0.44705883]\n",
      " ...\n",
      " [0.45490196 0.45882353 0.46666667 ... 0.6509804  0.65882355 0.6901961 ]\n",
      " [0.47843137 0.49803922 0.5058824  ... 0.6745098  0.67058825 0.69411767]\n",
      " [0.5058824  0.52156866 0.53333336 ... 0.69803923 0.6745098  0.69803923]], shape=(100, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-c92bdae2075f>:8: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "<ipython-input-10-c92bdae2075f>:10: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image,label in train_dataset.take(1):\n",
    "    print(image.shape)\n",
    "    img = image[0][:,:,0]\n",
    "    lbl = label[0][:,:,0]\n",
    "    plt.imshow(img,cmap=plt.cm.gray, interpolation='nearest')\n",
    "    plt.show()\n",
    "    plt.imshow(lbl,cmap=plt.cm.gray, interpolation='nearest')\n",
    "    plt.show()\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "\n",
    "#with strategy.scope():\n",
    "conv_encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(IMG_HEIGHT,IMG_WIDTH,1)),\n",
    "                tf.keras.layers.Conv2D(32, \n",
    "                                       kernel_size=5, \n",
    "                                       activation='relu'\n",
    "                                      ),\n",
    "                tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "                tf.keras.layers.Conv2D(32, \n",
    "                                       kernel_size=5,\n",
    "                                       activation='relu'\n",
    "                                      ),\n",
    "                tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(1000)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "conv_decoder = tf.keras.Sequential(\n",
    "            [   \n",
    "                tf.keras.layers.Dense(22*22*32, \n",
    "                                      activation='relu'\n",
    "                                     ),\n",
    "                tf.keras.layers.Reshape(target_shape=(22,22,32)),\n",
    "                tf.keras.layers.UpSampling2D(2, interpolation='nearest'),\n",
    "                tf.keras.layers.Conv2DTranspose(32, \n",
    "                                                kernel_size=5, \n",
    "                                                activation=\"relu\"\n",
    "                                               ),\n",
    "                tf.keras.layers.UpSampling2D(2, interpolation='nearest'),\n",
    "                tf.keras.layers.Conv2DTranspose(1, \n",
    "                                                kernel_size=5, \n",
    "                                                activation=\"sigmoid\"\n",
    "                                               )\n",
    "            ]            \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1000)\n",
      "(None, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "image_input = tf.keras.Input(shape=(IMG_HEIGHT,IMG_WIDTH,1))\n",
    "encoded = conv_encoder(image_input)\n",
    "print(encoded.shape)\n",
    "decoded = conv_decoder(encoded)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ae = tf.keras.Model(inputs=image_input, outputs=decoded)\n",
    "\n",
    "#conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
    "conv_ae.compile(loss=tf.keras.losses.MeanSquaredError(), \n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-4, decay=1e-4),\n",
    "        metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 100, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 1000)              15515464  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 100, 100, 1)       15529921  \n",
      "=================================================================\n",
      "Total params: 31,045,385\n",
      "Trainable params: 31,045,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#conv_ae(np.ones((1,100,100,1), dkaggletype=np.float32))\n",
    "\n",
    "conv_ae.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/425 [..............................] - ETA: 0s - loss: 0.0580 - mae: 0.2151WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0081s vs `on_train_batch_end` time: 0.0195s). Check your callbacks.\n",
      "425/425 [==============================] - 11s 26ms/step - loss: 0.0060 - mae: 0.0441 - val_loss: 0.0072 - val_mae: 0.0497\n",
      "Epoch 2/30\n",
      "425/425 [==============================] - 12s 27ms/step - loss: 0.0037 - mae: 0.0317 - val_loss: 0.0061 - val_mae: 0.0477\n",
      "Epoch 3/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 0.0028 - mae: 0.0278 - val_loss: 0.0048 - val_mae: 0.0386\n",
      "Epoch 4/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 0.0022 - mae: 0.0251 - val_loss: 0.0043 - val_mae: 0.0372\n",
      "Epoch 5/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 0.0019 - mae: 0.0229 - val_loss: 0.0038 - val_mae: 0.0331\n",
      "Epoch 6/30\n",
      "425/425 [==============================] - 13s 32ms/step - loss: 0.0016 - mae: 0.0212 - val_loss: 0.0037 - val_mae: 0.0321\n",
      "Epoch 7/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 0.0014 - mae: 0.0199 - val_loss: 0.0036 - val_mae: 0.0315\n",
      "Epoch 8/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 0.0012 - mae: 0.0189 - val_loss: 0.0035 - val_mae: 0.0312\n",
      "Epoch 9/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 0.0011 - mae: 0.0181 - val_loss: 0.0035 - val_mae: 0.0310\n",
      "Epoch 10/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 0.0011 - mae: 0.0175 - val_loss: 0.0035 - val_mae: 0.0307\n",
      "Epoch 11/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 9.9338e-04 - mae: 0.0169 - val_loss: 0.0034 - val_mae: 0.0303\n",
      "Epoch 12/30\n",
      "425/425 [==============================] - 12s 27ms/step - loss: 9.4569e-04 - mae: 0.0166 - val_loss: 0.0034 - val_mae: 0.0305\n",
      "Epoch 13/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 9.0699e-04 - mae: 0.0163 - val_loss: 0.0035 - val_mae: 0.0314\n",
      "Epoch 14/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 8.9439e-04 - mae: 0.0164 - val_loss: 0.0034 - val_mae: 0.0302\n",
      "Epoch 15/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 8.4161e-04 - mae: 0.0156 - val_loss: 0.0034 - val_mae: 0.0299\n",
      "Epoch 16/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 8.1291e-04 - mae: 0.0153 - val_loss: 0.0034 - val_mae: 0.0298\n",
      "Epoch 17/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 7.9180e-04 - mae: 0.0152 - val_loss: 0.0034 - val_mae: 0.0303\n",
      "Epoch 18/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 7.7019e-04 - mae: 0.0150 - val_loss: 0.0034 - val_mae: 0.0304\n",
      "Epoch 19/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 7.5161e-04 - mae: 0.0148 - val_loss: 0.0034 - val_mae: 0.0304\n",
      "Epoch 20/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 7.4012e-04 - mae: 0.0148 - val_loss: 0.0034 - val_mae: 0.0306\n",
      "Epoch 21/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 7.2550e-04 - mae: 0.0146 - val_loss: 0.0034 - val_mae: 0.0293\n",
      "Epoch 22/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 7.1724e-04 - mae: 0.0146 - val_loss: 0.0034 - val_mae: 0.0299\n",
      "Epoch 23/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 6.9563e-04 - mae: 0.0142 - val_loss: 0.0034 - val_mae: 0.0297\n",
      "Epoch 24/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 6.8222e-04 - mae: 0.0141 - val_loss: 0.0034 - val_mae: 0.0301\n",
      "Epoch 25/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 6.7036e-04 - mae: 0.0140 - val_loss: 0.0034 - val_mae: 0.0304\n",
      "Epoch 26/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 6.6032e-04 - mae: 0.0139 - val_loss: 0.0034 - val_mae: 0.0302\n",
      "Epoch 27/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 6.5256e-04 - mae: 0.0138 - val_loss: 0.0034 - val_mae: 0.0303\n",
      "Epoch 28/30\n",
      "425/425 [==============================] - 12s 29ms/step - loss: 6.4994e-04 - mae: 0.0139 - val_loss: 0.0034 - val_mae: 0.0298\n",
      "Epoch 29/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 6.3897e-04 - mae: 0.0137 - val_loss: 0.0034 - val_mae: 0.0298\n",
      "Epoch 30/30\n",
      "425/425 [==============================] - 12s 28ms/step - loss: 6.2935e-04 - mae: 0.0136 - val_loss: 0.0034 - val_mae: 0.0298\n"
     ]
    }
   ],
   "source": [
    "history = conv_ae.fit(train_dataset, epochs=30, validation_data=test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(history):\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-4e9635f71366>:10: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plot_training_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    print('plot image {}'.format(image.shape))\n",
    "    plt.imshow(image[:,:,0], cmap=plt.cm.gray, interpolation='nearest')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(model, images, n_images=5):\n",
    "    print('show reconstructions = {}'.format(images.shape))\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 3, 5))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 100, 1)\n",
      "show reconstructions = (1, 100, 100, 1)\n",
      "plot image (100, 100, 1)\n",
      "plot image (100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "test_images = list(test_dataset.take(1))[0]\n",
    "test_image = test_images[0]\n",
    "print(test_image.shape)\n",
    "show_reconstructions(conv_ae, test_image, n_images=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(img, output, diff, H, threshold, counter):\n",
    "    #print('inside plot, imgshape {}'.format(img.shape))\n",
    "    fig, (ax0, ax1, ax2,ax3) = plt.subplots(ncols=4, figsize=(10, 5))\n",
    "    ax0.set_axis_off()\n",
    "    ax1.set_axis_off()\n",
    "    ax2.set_axis_off()\n",
    "    \n",
    "    ax0.set_title('input image')\n",
    "    ax1.set_title('reconstructed image')\n",
    "    ax2.set_title('diff ')\n",
    "    ax3.set_title('anomalies')\n",
    "    \n",
    "    ax0.imshow(img, cmap=plt.cm.gray, interpolation='nearest') \n",
    "    ax1.imshow(output, cmap=plt.cm.gray, interpolation='nearest')   \n",
    "    ax2.imshow(diff, cmap=plt.cm.viridis, vmin=0, vmax=255, interpolation='nearest')  \n",
    "    ax3.imshow(img, cmap=plt.cm.gray, interpolation='nearest')\n",
    "    \n",
    "    x,y = np.where(H > threshold)\n",
    "    ax3.scatter(y,x,color='red',s=0.1) \n",
    "\n",
    "    plt.axis('off')\n",
    "    \n",
    "    fig.savefig('images/{:0>3d}.png'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-75407e109c37>:3: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, (ax0, ax1, ax2,ax3) = plt.subplots(ncols=4, figsize=(10, 5))\n"
     ]
    }
   ],
   "source": [
    "threshold = 4*255\n",
    "counter = 0;\n",
    "\n",
    "for image, _  in test_dataset:\n",
    "    counter = counter + 1\n",
    "    output = conv_ae.predict(image)\n",
    "    output = tf.multiply(output,255.)\n",
    "    img = tf.multiply(tf.cast(image, tf.float32), 255.)\n",
    "    diff = tf.subtract(output,img)\n",
    "    tmp = diff[0,:,:,0]\n",
    "    H = signal.convolve2d(tmp, np.ones((4,4)), mode='same')\n",
    "    plot(img[0,:,:,0], output[0,:,:,0], diff[0,:,:,0], H, threshold, counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images/001.png', 'images/002.png', 'images/003.png', 'images/004.png', 'images/005.png', 'images/006.png', 'images/007.png', 'images/008.png', 'images/009.png', 'images/010.png']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    " \n",
    "# Create the frames\n",
    "frames = []\n",
    "imgs = sorted(glob.glob('images/*.png'))\n",
    "print(imgs[:10])\n",
    "for i in imgs:\n",
    "    new_frame = Image.open(i)\n",
    "    frames.append(new_frame)\n",
    " \n",
    "# Save into a GIF file that loops forever\n",
    "frames[0].save('result.gif', format='GIF',\n",
    "               append_images=frames[1:],\n",
    "               save_all=True,\n",
    "               duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf2env)",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
